---
title: "Data Science"
date: 2025-08-30T07:44:47+02:00
---


maximum likelihood和least squares的关系？

我们假设有一些数据是由多项式方程生成的，但是采样过程中会有高斯噪声。我们想要对这些数据线性回归得到一个可以预测的模型。在这个过程中我们关注一个目标函数：sum-of-squares error（SSE）。目的是使这个函数最小化。
SSE是目标函数，而最小二乘是方法。最小二乘（least squares）思想是，找到一组参数使得预测值和真实值之间的平方误差和最小。为什么叫二乘？乘有“次方”的意思，二乘表示二次方。SSE里就是用的误差二次方和。$t_n$表示在$x_n$处的观测值。$x_n$处的真实值（模型预测）为$y(x_n;\mathbf{w})$。这里$x_n$和$\mathbf{w}$用分号隔开，而不用逗号，这是为什么？逗号表示两者都是自变量。而实际该函数只有$x_n$为自变量。在模型训练完毕以后，$\mathbf{w}$被确定，因此$\mathbf{w}$作为学习的参数在训练之后是固定的，它不是自变量。$\\mathbf{w}$这里用向量表示，因为它包含的是一些权重，而非单一数值。
