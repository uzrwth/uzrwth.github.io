.TL
The running example

.PP
The underlying data is generated by: sin(2πx) + noise(Gaussian)

Probability theory provides a framework for expressing uncertainty in a precise and quantitative manner.

Decision theory allows us to exploit this probabilistic representation in order to make predictions that are optimal according to appropriate criteria.

Polynomial fitting:


[[polynomial_fitting.png]]

It is linear in the unknown parameters. Such models are called linear models.

Now, how do we train and find the values of the unknown parameters?

The values of the coefficients will be determined by fitting the polynomial to the training data.

This can be done by minimizing an error function that measures the misfit between the function and the training set data points.

One simple choice of error function is the sum of the squares of the errors between the predictions for each data point and the corresponding target values.

Minmize the error function:

[[error_function.png]]

We can solve the curve fitting problem by choosing the value of w for which E(w) is as small as possible.

There remains the problem of choosing the order M of the polynomial.

By choosing a large order M, we trained an over-fitting model.

RMS: root mean square

[[rms.png]]

The division by N allows us to compare different sizes of data sets on an equal footing, and the square root ensures that E_RMS is measured on the same scale (and in the same units) as the target variable t.


We might suppose that the best predictor for new data would be the function sin(2πx) from which the data was generated (and this is indeed the case)

A power series expansion of the function sin(2πx) contains terms of all orders, so we might expect that results should improve monotonically as we increase M.

However, by increasing M, we get over-fitting, which is counter intuitive.

We can gain some insight into the problem by examine the vlaues of the coefficients w* obtained from polynomials of various order.

As M increases, the magnitude of the coefficients typically gets larger.

[[coefficients_gets_larger.png]]

What is happening is that the more flexible polynomials with larger values of M are becoming increasingly tuned to the random noise on the target values.

For a given model complexity, the over-fitting problem become less severe as the size of the data set increases.

[[model_complexity.png]]


But why is least squares a good choice for an error function?

We shall see that the least squares approach to finding the model parameters represents a specific case of maximum likelihood, and that the over-fitting problem can be understood as a general property of maximum likelihood.

By adopting a Bayesian approach, the over-fitting problem can be avoided.

We shall see that there is no difficulty from a Bayesian perspective in employing models for which the number of parameters greatly exceeds the number of data points. Indeed, in a Bayesian model the effective number of parameters adapts automatically to the size of the data set.

One technique that is often used to control the over-fitting phenomenon in such cases is that of regularization, which involves adding a penalty term to the error function in order to discourage the coefficients from reaching large values.


[[regularization_and_overfitting.png]]


Different penalties:

[[different_penalties.png]]

Large penalties turn the prediction curves into smooth ones.

Note that often the coefficient w_0 is omitted from the regularizer because its inclusion causes the results to depend on the choice of origin for the target variable.

The particular case of a quadratic regularizer is called ridge regression. In the context of neural networks, this approach is known as weight decay.


[[coefficients_after_regularized.png]]

The regularization has the desired effect of reducing the magnitude of the coefficients.

The impact of the regularization term on the generalization error can be seen by plotting the value of the RMS error for both training and test sets against ln λ

We see that in effect  λ now controls the effective complexity of the model and hence determines the degree of over-fitting.

[[effective_model_complexity.png]]

The issue of model complexity is an important one.

How do we optimize the model complexity (either M or λ )?

Partitioning it into a training set, used to determine the coefficients w, and a separate validation set, also called a hold-out set, used to optimize the model complexity. In many cases, however, this will prove to be too wasteful of valuable training data.


.SH
But why is least squares a good choice for an error function?

.PP
With probability theory, we can express our uncertainty over the value of the target variable using a probability distribution.

We shall assume that, given the value of x, the corresponding value of t has a Gaussian distribution with a mean equal to the value y(x, w) of the polynomial curve.

[[t_is_a_distribution.png]]

[[gaussian_conditional.png]]


Conclusion: the sum-of-squares error function has arisen as a consequence of maximizing likelihood under the assumption of a Gaussian noise distribution.



.SH
What is Bayesian approach?

.PP

